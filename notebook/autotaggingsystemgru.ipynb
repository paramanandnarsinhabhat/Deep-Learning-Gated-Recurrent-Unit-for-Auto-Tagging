{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string matching\n",
    "import re \n",
    "\n",
    "#reading files\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#handling html data\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = 'data/archive (2).zip'\n",
    "\n",
    "# Specify the directory to extract to\n",
    "extract_to_dir = 'data/unzipped_contents'\n",
    "\n",
    "# Create a directory to extract to if it doesn't exist\n",
    "os.makedirs(extract_to_dir, exist_ok=True)\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents into the directory\n",
    "    zip_ref.extractall(extract_to_dir)\n",
    "    \n",
    "    # List the contents of the extracted folder\n",
    "    print(f\"Contents of the zip file '{zip_file_path}':\")\n",
    "    for file_name in zip_ref.namelist():\n",
    "        print(file_name)\n",
    "\n",
    "# Now you can access files inside the unzipped directory\n",
    "# For example, to open a file:\n",
    "# with open(os.path.join(extract_to_dir, 'yourfile.txt'), 'r') as file:\n",
    "#     print(file.read())\n",
    "\n",
    "# load the stackoverflow questions dataset\n",
    "questions_df = pd.read_csv('data/unzipped_contents/Questions.csv',encoding='latin-1')\n",
    "\n",
    "#print first 5 rows\n",
    "print(questions_df.head())\n",
    "\n",
    "#data/unzipped_contents/\n",
    "# load the tags dataset\n",
    "tags_df = pd.read_csv('data/unzipped_contents/Tags.csv')\n",
    "\n",
    "print(tags_df.head())\n",
    "\n",
    "#print first 5 rows\n",
    "questions_df.head()\n",
    "\n",
    "# Text Cleaning\n",
    "# Let's define a function to clean the text data.\n",
    "\n",
    "def cleaner(text):\n",
    "\n",
    "  text = BeautifulSoup(text).get_text()\n",
    "  \n",
    "  # fetch alphabetic characters\n",
    "  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "  # convert text to lower case\n",
    "  text = text.lower()\n",
    "\n",
    "  # split text into tokens to remove whitespaces\n",
    "  tokens = text.split()\n",
    "\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "# call preprocessing function\n",
    "questions_df['cleaned_text'] = questions_df['Body'].apply(cleaner)\n",
    "\n",
    "print(questions_df['Body'][1])\n",
    "\n",
    "print(questions_df['cleaned_text'][1])\n",
    "\n",
    "# Merge Tags with Questions\n",
    "\n",
    "# count of unique tags\n",
    "len(tags_df['Tag'].unique())\n",
    "\n",
    "print(len(tags_df['Tag'].unique()))\n",
    "\n",
    "print(tags_df['Tag'].value_counts())\n",
    "\n",
    "\n",
    "# remove \"-\" from the tags\n",
    "tags_df['Tag']= tags_df['Tag'].apply(lambda x:re.sub(\"-\",\" \",x))\n",
    "\n",
    "# group tags Id wise\n",
    "tags_df = tags_df.groupby('Id').apply(lambda x:x['Tag'].values).reset_index(name='tags')\n",
    "tags_df.head()\n",
    "\n",
    "print(tags_df.head())\n",
    "\n",
    "# merge tags and questions\n",
    "df = pd.merge(questions_df,tags_df,how='inner',on='Id')\n",
    "\n",
    "df = df[['Id','Body','cleaned_text','tags']]\n",
    "df.head()\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Dataset Preparation\n",
    "# check frequency of occurence of each tag\n",
    "freq= {}\n",
    "for i in df['tags']:\n",
    "  for j in i:\n",
    "    if j in freq.keys():\n",
    "      freq[j] = freq[j] + 1\n",
    "    else:\n",
    "      freq[j] = 1\n",
    "\n",
    "#Let's find out the most frequent tags.\n",
    "# sort the dictionary in descending order\n",
    "freq = dict(sorted(freq.items(), key=lambda x:x[1],reverse=True))\n",
    "\n",
    "freq.items()\n",
    "\n",
    "print(freq.items())\n",
    "\n",
    "# Top 10 most frequent tags\n",
    "common_tags = list(freq.keys())[:10]\n",
    "common_tags\n",
    "\n",
    "print(common_tags)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for i in range(len(df['tags'])):\n",
    "  \n",
    "  temp=[]\n",
    "  for j in df['tags'][i]:\n",
    "    if j in common_tags:\n",
    "      temp.append(j)\n",
    "\n",
    "  if(len(temp)>1):\n",
    "    x.append(df['cleaned_text'][i])\n",
    "    y.append(temp)\n",
    "\n",
    "# number of questions left\n",
    "len(x)\n",
    "\n",
    "print(len(x))\n",
    "\n",
    "print(y[:10])\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    " \n",
    "y = mlb.fit_transform(y)\n",
    "y.shape\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "y[0,:]\n",
    "\n",
    "print(y[0,:])\n",
    "\n",
    "print(mlb.classes_)\n",
    "\n",
    "#We can now split the dataset into training set and validation set. \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(x, y, test_size=0.2, random_state=0,shuffle=True)\n",
    "\n",
    "# Text Representation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "#prepare a tokenizer\n",
    "x_tokenizer = Tokenizer() \n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)\n",
    "\n",
    "print(x_tokenizer.word_index)\n",
    "\n",
    "print(len(x_tokenizer.word_index))\n",
    "\n",
    "thresh = 3\n",
    "\n",
    "cnt=0\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "  if value>=thresh:\n",
    "    cnt=cnt+1\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "#Over 12,000 tokens have appeared three times or more in the training set.\n",
    "# prepare the tokenizer again\n",
    "x_tokenizer = Tokenizer(num_words=cnt,oov_token='unk')\n",
    "\n",
    "#prepare vocabulary\n",
    "x_tokenizer.fit_on_texts(x_tr)\n",
    "\n",
    "'''\n",
    "Now that we have encoded every token to an integer, let's convert the text sequences to integer sequences. After that we will pad the integer sequences to the maximum sequence length, i.e., 100.\n",
    "\n",
    "'''\n",
    "#define threshold for maximum length of a setence\n",
    "max_len=100\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding up with zero \n",
    "x_tr_seq = pad_sequences(x_tr_seq,  padding='post', maxlen=max_len)\n",
    "x_val_seq = pad_sequences(x_val_seq, padding='post', maxlen=max_len)\n",
    "\n",
    "#no. of unique words\n",
    "x_voc_size = x_tokenizer.num_words + 1\n",
    "x_voc_size\n",
    "\n",
    "print(x_voc_size)\n",
    "\n",
    "x_tr_seq[0]\n",
    "\n",
    "print(x_tr_seq[0])\n",
    "\n",
    "# Model Building\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "### Define Model Architecture\n",
    "#sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#embedding layer\n",
    "model.add(Embedding(x_voc_size, 50, trainable = True, input_shape=(max_len,),mask_zero=True))\n",
    "\n",
    "#lstm \n",
    "model.add(GRU(128))\n",
    "\n",
    "#dense layer\n",
    "model.add(Dense(128,activation='relu')) \n",
    "\n",
    "#output layer\n",
    "model.add(Dense(10,activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#define optimizer and loss\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "\n",
    "#checkpoint to save best model during training\n",
    "mc = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "### Train the Model\n",
    "#train the model \n",
    "model.fit(x_tr_seq, y_tr, batch_size=128, epochs=10, verbose=1, validation_data=(x_val_seq, y_val), callbacks=[mc])\n",
    "\n",
    "# Model Predictions \n",
    "# load weights into new model\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "#predict probabilities\n",
    "pred_prob = model.predict(x_val_seq)\n",
    "\n",
    "pred_prob[0]\n",
    "\n",
    "print(pred_prob[0])\n",
    "\n",
    "'''\n",
    "The predictions are in terms of probabilities for each of the 10 tags. Hence we need to have a threshold value to convert these probabilities to 0 or 1.\n",
    "\n",
    "Let's specify a set of candidate threshold values. We will select the threshold value that performs the best for the validation set.\n",
    "\n",
    "'''\n",
    "#define candidate threshold values\n",
    "threshold  = np.arange(0,0.5,0.01)\n",
    "threshold\n",
    "\n",
    "print(threshold)\n",
    "\n",
    "'''\n",
    "Let's define a function that takes a threshold value and uses it to convert probabilities into 1 or 0.\n",
    "'''\n",
    "\n",
    "# convert probabilities into classes or tags based on a threshold value\n",
    "def classify(pred_prob,thresh):\n",
    "  y_pred_seq = []\n",
    "\n",
    "  for i in pred_prob:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "      if j>=thresh:\n",
    "        temp.append(1)\n",
    "      else:\n",
    "        temp.append(0)\n",
    "    y_pred_seq.append(temp)\n",
    "\n",
    "  return y_pred_seq\n",
    "\n",
    "from sklearn import metrics\n",
    "score=[]\n",
    "\n",
    "#convert to 1 array\n",
    "y_true = np.array(y_val).ravel() \n",
    "\n",
    "for thresh in threshold:\n",
    "    \n",
    "    #classes for each threshold\n",
    "    y_pred_seq = classify(pred_prob,thresh) \n",
    "\n",
    "    #convert to 1d array\n",
    "    y_pred = np.array(y_pred_seq).ravel()\n",
    "\n",
    "    score.append(metrics.f1_score(y_true,y_pred))\n",
    "\n",
    "# find the optimal threshold\n",
    "opt = threshold[score.index(max(score))]\n",
    "opt\n",
    "\n",
    "print(opt)\n",
    "\n",
    "# Model Evaluation\n",
    "#predictions for optimal threshold\n",
    "y_pred_seq = classify(pred_prob,opt)\n",
    "y_pred = np.array(y_pred_seq).ravel()\n",
    "\n",
    "print(metrics.classification_report(y_true,y_pred))\n",
    "\n",
    "y_pred = mlb.inverse_transform(np.array(y_pred_seq))\n",
    "y_true = mlb.inverse_transform(np.array(y_val))\n",
    "\n",
    "df = pd.DataFrame({'comment':x_val,'actual':y_true,'predictions':y_pred})\n",
    "\n",
    "print(df.sample(10))\n",
    "\n",
    "# Inference\n",
    "def predict_tag(comment):  \n",
    "  text=[]\n",
    "\n",
    "  #preprocess  \n",
    "  text = [cleaner(comment)]\n",
    "\n",
    "  #convert to integer sequences\n",
    "  seq = x_tokenizer.texts_to_sequences(text)\n",
    "\n",
    "  #pad the sequence\n",
    "  pad_seq = pad_sequences(seq,  padding='post', maxlen=max_len)\n",
    "\n",
    "  #make predictions\n",
    "  pred_prob = model.predict(pad_seq)\n",
    "  classes = classify(pred_prob,opt)[0]\n",
    "  \n",
    "  classes = np.array([classes])\n",
    "  classes = mlb.inverse_transform(classes)  \n",
    "  return classes\n",
    "\n",
    "comment = \"For example, in the case of logistic regression, the learning function is a Sigmoid function that tries to separate the 2 classes\"\n",
    "\n",
    "print(\"Comment:\",comment)\n",
    "print(\"Predicted Tags:\",predict_tag(comment))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
